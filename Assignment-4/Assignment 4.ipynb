{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSc 8830: Computer Vision - Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement an application (must run on web or as an app on mobile device) using the stereo camera where it will recognize, track and estimate dimensions (at least 2D) of any object within 3m distance and inside field-of-view to the camera. You can use barcodes or text recognition tools for identification. However, the entire object must be tracked (not just the barcode or text). Machine/Deep learning tools are NOT allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/flgh8s7960bc7q380pyn0c6m0000gn/T/ipykernel_5641/1840294533.py:203: DeprecationWarning: setConfidenceThreshold() is deprecated, Use 'initialConfig.setConfidenceThreshold()' instead\n",
      "  stereo.setConfidenceThreshold(255)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1844301031A5061300] [0.1.1] [0.925] [MonoCamera(1)] [error] OV7251 only supports THE_480_P/THE_400_P resolutions, defaulting to THE_480_P\n",
      "[1844301031A5061300] [0.1.1] [0.926] [MonoCamera(2)] [error] OV7251 only supports THE_480_P/THE_400_P resolutions, defaulting to THE_480_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/flgh8s7960bc7q380pyn0c6m0000gn/T/ipykernel_5641/1840294533.py:256: DeprecationWarning: Device(pipeline) starts the pipeline automatically. Use Device() and startPipeline(pipeline) otherwise\n",
      "  device.startPipeline()\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import imutils\n",
    "import numpy as np\n",
    "import depthai as dai\n",
    "from imutils import perspective\n",
    "from imutils import contours\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "\n",
    "# Function to detect QR codes in an image\n",
    "def detect_qr_codes(frame):\n",
    "    qcd = cv2.QRCodeDetector()\n",
    "    ret_qr, decoded_info, points, _ = qcd.detectAndDecodeMulti(frame)\n",
    "    if ret_qr:\n",
    "        for s, p in zip(decoded_info, points):\n",
    "            if s:\n",
    "                # Draw rectangle\n",
    "                frame = cv2.polylines(frame, [p.astype(int)], True, (0, 255, 0), 8)\n",
    "                # Add text\n",
    "                cv2.putText(frame, s, (int(p[0][0]), int(p[0][1])-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "                frame = cv2.polylines(frame, [p.astype(int)], True, color, 8)\n",
    "    return frame\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self):\n",
    "        # Dictionary to store the center points of the tracked objects\n",
    "        self.object_centers = {}\n",
    "        # Counter to keep track of object IDs\n",
    "        self.object_id_count = 0\n",
    "\n",
    "    def update(self, frame):\n",
    "        # Extract Region of Interest (ROI)\n",
    "        roi = frame\n",
    "\n",
    "        # Background subtractor for object detection\n",
    "        object_detector = cv2.createBackgroundSubtractorMOG2(history=100, varThreshold=40)\n",
    "\n",
    "        # 1. Object Detection\n",
    "        mask = object_detector.apply(roi)\n",
    "        _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        detections = []\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 100:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                detections.append([x, y, w, h])\n",
    "\n",
    "        # 2. Object Tracking\n",
    "        tracked_boxes_ids = self._track_objects(detections)\n",
    "        for box_id in tracked_boxes_ids:\n",
    "            x, y, w, h, obj_id = box_id\n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "        return roi\n",
    "\n",
    "    def _track_objects(self, object_rectangles):\n",
    "        # List to store object bounding boxes and IDs\n",
    "        tracked_objects = []\n",
    "\n",
    "        # Update center points of existing objects or assign new IDs\n",
    "        for rect in object_rectangles:\n",
    "            x, y, w, h = rect\n",
    "            cx = (x + x + w) // 2\n",
    "            cy = (y + y + h) // 2\n",
    "\n",
    "            # Check if the object was detected previously\n",
    "            object_detected = False\n",
    "            for obj_id, center in self.object_centers.items():\n",
    "                dist = math.hypot(cx - center[0], cy - center[1])\n",
    "\n",
    "                # If distance is within a threshold, update center and ID\n",
    "                if dist < 25:\n",
    "                    self.object_centers[obj_id] = (cx, cy)\n",
    "                    tracked_objects.append([x, y, w, h, obj_id])\n",
    "                    object_detected = True\n",
    "                    break\n",
    "\n",
    "            # If new object detected, assign a new ID\n",
    "            if not object_detected:\n",
    "                self.object_centers[self.object_id_count] = (cx, cy)\n",
    "                tracked_objects.append([x, y, w, h, self.object_id_count])\n",
    "                self.object_id_count += 1\n",
    "\n",
    "        # Clean up dictionary by removing unused IDs\n",
    "        new_object_centers = {}\n",
    "        for obj_bb_id in tracked_objects:\n",
    "            _, _, _, _, obj_id = obj_bb_id\n",
    "            center = self.object_centers[obj_id]\n",
    "            new_object_centers[obj_id] = center\n",
    "\n",
    "        # Update dictionary with used IDs\n",
    "        self.object_centers = new_object_centers.copy()\n",
    "        return tracked_objects\n",
    "\n",
    "\n",
    "# Create object tracker instance\n",
    "tracker = ObjectTracker()\n",
    "\n",
    "def midpoint(ptA, ptB):\n",
    "    return ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\n",
    "\n",
    "class ObjectDimensionMarker:\n",
    "    def __init__(self, object_width_inches):\n",
    "        self.object_width_inches = object_width_inches\n",
    "        self.pixelsPerMetric = None\n",
    "\n",
    "    def mark_object_dimensions(self, gray_frame):\n",
    "        # convert the frame to grayscale and blur it slightly\n",
    "        gray = cv2.GaussianBlur(gray_frame, (7, 7), 0)\n",
    "\n",
    "        # perform edge detection, then perform a dilation + erosion to\n",
    "        # close gaps in between object edges\n",
    "        edged = cv2.Canny(gray, 50, 100)\n",
    "        edged = cv2.dilate(edged, None, iterations=1)\n",
    "        edged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "        # find contours in the edge map\n",
    "        cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "                                cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "        # sort the contours from left-to-right\n",
    "        (cnts, _) = contours.sort_contours(cnts)\n",
    "        \n",
    "        # Initialize result frame\n",
    "        result_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # loop over the contours individually\n",
    "        for c in cnts:\n",
    "            # if the contour is not sufficiently large, ignore it\n",
    "            if cv2.contourArea(c) < 100:\n",
    "                continue\n",
    "\n",
    "            # compute the rotated bounding box of the contour\n",
    "            box = cv2.minAreaRect(c)\n",
    "            box = cv2.boxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n",
    "            box = np.array(box, dtype=\"int\")\n",
    "\n",
    "            # order the points in the contour and draw the outline of the rotated bounding box\n",
    "            box = perspective.order_points(box)\n",
    "            cv2.drawContours(result_frame, [box.astype(\"int\")], -1, (0, 255, 0), 2)\n",
    "\n",
    "            # loop over the original points and draw them\n",
    "            for (x, y) in box:\n",
    "                cv2.circle(result_frame, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "            # unpack the ordered bounding box\n",
    "            (tl, tr, br, bl) = box\n",
    "\n",
    "            # compute midpoints\n",
    "            (tltrX, tltrY) = midpoint(tl, tr)\n",
    "            (blbrX, blbrY) = midpoint(bl, br)\n",
    "            (tlblX, tlblY) = midpoint(tl, bl)\n",
    "            (trbrX, trbrY) = midpoint(tr, br)\n",
    "\n",
    "            # draw midpoints\n",
    "            cv2.circle(result_frame, (int(tltrX), int(tltrY)), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(result_frame, (int(blbrX), int(blbrY)), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(result_frame, (int(tlblX), int(tlblY)), 5, (255, 0, 0), -1)\n",
    "            cv2.circle(result_frame, (int(trbrX), int(trbrY)), 5, (255, 0, 0), -1)\n",
    "\n",
    "            # draw lines between the midpoints\n",
    "            cv2.line(result_frame, (int(tltrX), int(tltrY)), (int(blbrX), int(blbrY)),\n",
    "                     (255, 0, 255), 2)\n",
    "            cv2.line(result_frame, (int(tlblX), int(tlblY)), (int(trbrX), int(trbrY)),\n",
    "                     (255, 0, 255), 2)\n",
    "\n",
    "            # compute the Euclidean distance between the midpoints\n",
    "            dA = dist.euclidean((tltrX, tltrY), (blbrX, blbrY))\n",
    "            dB = dist.euclidean((tlblX, tlblY), (trbrX, trbrY))\n",
    "\n",
    "            # if the pixels per metric has not been initialized, then\n",
    "            # compute it as the ratio of pixels to supplied metric\n",
    "            if self.pixelsPerMetric is None:\n",
    "                self.pixelsPerMetric = dB / self.object_width_inches\n",
    "\n",
    "            # compute the size of the object in centimeters\n",
    "            dimA_cm = dA / self.pixelsPerMetric * 2.54  # converting inches to centimeters\n",
    "            dimB_cm = dB / self.pixelsPerMetric * 2.54\n",
    "\n",
    "            # draw the object sizes on the image\n",
    "            cv2.putText(result_frame, \"{:.1f}cm\".format(dimA_cm),\n",
    "                        (int(tltrX - 15), int(tltrY - 10)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.65, (255, 255, 255), 2)\n",
    "            cv2.putText(result_frame, \"{:.1f}cm\".format(dimB_cm),\n",
    "                        (int(trbrX + 10), int(trbrY)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.65, (255, 255, 255), 2)\n",
    "\n",
    "        return result_frame\n",
    "    \n",
    "# Initialize object dimension marker\n",
    "object_marker = ObjectDimensionMarker(object_width_inches=2.24)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a node for the stereo camera\n",
    "stereo = pipeline.createStereoDepth()\n",
    "stereo.setConfidenceThreshold(255)\n",
    "\n",
    "# Define a node for the left camera\n",
    "left = pipeline.createMonoCamera()\n",
    "left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_720_P)\n",
    "\n",
    "# Define a node for the right camera\n",
    "right = pipeline.createMonoCamera()\n",
    "right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_720_P)\n",
    "\n",
    "# Connect left and right camera outputs to the stereo node\n",
    "left.out.link(stereo.left)\n",
    "right.out.link(stereo.right)\n",
    "\n",
    "# Define a node for the output\n",
    "xoutDepth = pipeline.createXLinkOut()\n",
    "xoutDepth.setStreamName(\"depth\")\n",
    "\n",
    "# Link stereo camera output to the output node\n",
    "stereo.depth.link(xoutDepth.input)\n",
    "\n",
    "# Define a node to get left camera frames\n",
    "xoutLeft = pipeline.createXLinkOut()\n",
    "xoutLeft.setStreamName(\"left\")\n",
    "\n",
    "# Link left camera output to the output node\n",
    "left.out.link(xoutLeft.input)\n",
    "\n",
    "# Define a node to get right camera frames\n",
    "xoutRight = pipeline.createXLinkOut()\n",
    "xoutRight.setStreamName(\"right\")\n",
    "\n",
    "# Link right camera output to the output node\n",
    "right.out.link(xoutRight.input)\n",
    "\n",
    "# Define a source - color camera\n",
    "cam = pipeline.createColorCamera()\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "\n",
    "# Create RGB output\n",
    "xout = pipeline.createXLinkOut()\n",
    "xout.setStreamName(\"rgb\")\n",
    "cam.video.link(xout.input)\n",
    "\n",
    "# Connect to the device\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Output queues\n",
    "    depthQueue = device.getOutputQueue(name=\"depth\", maxSize=4, blocking=False)\n",
    "    leftQueue = device.getOutputQueue(name=\"left\", maxSize=4, blocking=False)\n",
    "    rightQueue = device.getOutputQueue(name=\"right\", maxSize=4, blocking=False)\n",
    "    rgbQueue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Start the pipeline\n",
    "    device.startPipeline()\n",
    "\n",
    "    # OpenCV setup\n",
    "    camera_id = 0\n",
    "    delay = 1\n",
    "    window_name = 'Object Tracking & Recgnition through QR Code'\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "\n",
    "    while True:\n",
    "        # Get the depth frame\n",
    "        inDepth = depthQueue.get()\n",
    "\n",
    "        # Get the left camera frame\n",
    "        inLeft = leftQueue.get()\n",
    "\n",
    "        # Get the right camera frame\n",
    "        inRight = rightQueue.get()\n",
    "\n",
    "        # Get the rgb camera frame\n",
    "        inSrc = rgbQueue.get()\n",
    "\n",
    "        # Access the depth data\n",
    "        depthFrame = inDepth.getFrame()\n",
    "\n",
    "        # Access the left camera frame\n",
    "        leftFrame = inLeft.getCvFrame()\n",
    "\n",
    "        # Access the right camera frame\n",
    "        rightFrame = inRight.getCvFrame()\n",
    "        \n",
    "        # Data is originally represented as a flat 1D array, it needs to be converted into HxW form\n",
    "        rgbFrame = inSrc.getCvFrame()\n",
    "\n",
    "        '''# Resize frames if necessary to have the same size\n",
    "        width = max(leftFrame.shape[1], rightFrame.shape[1])\n",
    "        height = max(leftFrame.shape[0], rightFrame.shape[0])\n",
    "        leftFrame = cv2.resize(leftFrame, (width, height))\n",
    "        rightFrame = cv2.resize(rightFrame, (width, height))\n",
    "\n",
    "        # Convert to anaglyph (red-cyan)\n",
    "        anaglyph = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        anaglyph[:, :, 0] = rightFrame[:, :]\n",
    "        anaglyph[:, :, 1] = leftFrame[:, :]\n",
    "        anaglyph[:, :, 2] = leftFrame[:, :]'''\n",
    "\n",
    "        # Combine left and right frames horizontally\n",
    "        stereoFrame = cv2.hconcat([leftFrame, rightFrame])\n",
    "\n",
    "        # Detect QR codes in the stereo frame\n",
    "        stereoFrame_with_qr = detect_qr_codes(stereoFrame)\n",
    "\n",
    "        # Perform object detection and tracking\n",
    "        stereoFrame_with_objects_qr = tracker.update(stereoFrame_with_qr)\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(rgbFrame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        try:\n",
    "            # Mark object dimensions\n",
    "            marked_frame = object_marker.mark_object_dimensions(gray_frame)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # Display stereo vision with QR code detection\n",
    "        cv2.imshow(\"Stereo Vision (Right & Left)\", stereoFrame_with_objects_qr)\n",
    "        cv2.imshow(\"RGB Camera\", marked_frame)\n",
    "        #cv2.imshow(\"Stereoscopic Vision\", anaglyph)\n",
    "        #v2.imshow(\"Depth\", depthFrame)\n",
    "\n",
    "        # Exit loop by pressing 'q'\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release OpenCV resources\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the DepthAI SDK or use ORB3-Visual SLAM (https://github.com/UZ-SLAMLab/ORB_SLAM3) to execute the scripts on your depth camera and run experiments in two different locations. Provide snapshots of your SLAM output and what limitations/corner cases do you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
